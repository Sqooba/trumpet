
The strongest requirement behind this project is to be completely decoupled from the NameNode for reading the transactions 
and streaming them to multiple clients.

The selected approach is thus to read the edits log file directly from the local filesystem from either the NameNode or, 
more recommended, the JournalNode, and stream the interesting transactions into [Kafka](http://kafka.apache.org). 
Many clients might in turn read from Kafka the complete set of transactions. The process of reading the edits log 
is independent from the NameNode or JournalNode and thus non-intrusive at all. 
And the Kafka cluster can be scaled out independently.

![Trumpet Flow Diagram 1](trumpet-flow.png "Trumpet Flow Diagram 1.0")

## Poll the edit log directory

Because Trumpet is reading from outside the NameNode / JournalNode process, the easiest solution is to poll the edit log 
directory, to find the latest edit log file and read the transaction from there. 
This edit log directory is given by the configuration option `dfs.journalnode.name.dir` (`dfs.namenode.name.dir` respectively 
for the NameNode), and is later referred as `dfs.*.name.dir`.

Based on naming convention of the edit log dir (read more on that here: http://hortonworks.com/blog/hdfs-metadata-directories-explained/), 
it's straight forward to find the file containing a given transaction and resuming the read from this transaction. 
Hadoop source code provides all the primitive functions to achieve that in few lines of code.


## Publish into Kafka

Kafka publisher subscriber model is a good fit to push the transactions into and allow several clients to read from. 
In current implementation supports [Kafka 0.8.2.1](https://archive.apache.org/dist/kafka/0.8.2.1/RELEASE_NOTES.html) 
and [Kafka 0.8.1.1](https://archive.apache.org/dist/kafka/0.8.1.1/RELEASE_NOTES.html). 
The topic contains one single partition to guarantee consistent ordering of the transactions. 
This might change in the future. Several replica are however strongly recommended.

Trumpet requires to have the topic created in advance. The default topic name is `hdfs.inotify.events`.
An example of topic creation would be:

* Kakfa 0.8.1
```
$ bin/kafka-topic.sh --create --zookeeper <zk_ip:2181> --replication-factor 4 --partitions 1 --topic hdfs.inotify.events
```
* Kakfa 0.8.2
```
$ kafka-topics --create --zookeeper <zk_ip:2181> --replication-factor 4 --partitions 1 --topic hdfs.inotify.events
```


Of course the scalability of the client applications reading the transactions will be influenced by 
the scalability of your Kafka cluster. Trumpet does not guarantee any transaction or reader persistence 
to the client application. The work is delegated to the clients.


## Leader Election

Hadoop runs with HA in mind, and this project follows the same concept. Trumpet is designed to run alongside the JournalNode 
(or NameNode) process, reading from the local `dfs.*.name.dir` directory. The idea is to run one Trumpet process per JournalNode, 
the processes running a leader election in Zookeeper using [Curator](http://curator.apache.org/) 
[recipe](http://curator.apache.org/curator-recipes/leader-election.html) to guarantee only one active process 
at the same time. The processes are also monitoring the JournalNode process, and release the leadership 
if the JournalNode process died.


## Resume from previous run

Once a Trumpet process become active (leader), it will find out from Kafka which was the latest published transaction, 
resuming the operation from there (or, in case of prolonged downtime, resume with the first found transaction in 
the local `dfs.*.name.dir` directory.


## Rolling upgrade

In case of production usage of Trumpet, rolling upgrades need to be addressed. With its leader election 
feature, Trumpet rolling upgrade is as easy as upgrading one Trumpet worker at the time. 
A tool which tells you who are the Trumpet workers and which one is active is also provided, 
see below.


## Zookeeper separation

Zookeeper is a critical component of the Hadoop infrastructure, and it's common to split Zookeeper cluster, 
one for the infrastructure components, like Kafka, NameNode etc... and another Zookeeper for more 
user-space applications.
In Trumpet, you can either use one Zookeeper cluster, or split the Zookeeper usages between 
Kafka discovery and leader election. Use:

* `--zk.connect` if you have one Zookeeper cluster
* `--zk.connect.kafka` and `--zk.connect.user` if you have two Zookeeper clusters.

